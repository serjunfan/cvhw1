{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b65981b-e09f-4bb2-854d-825ec473ab2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting to_coco100.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile to_coco100.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "MAX_N = 100\n",
    "\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"Person\"},\n",
    "    {\"id\": 1, \"name\": \"Ear\"},\n",
    "    {\"id\": 2, \"name\": \"Earmuffs\"},\n",
    "    {\"id\": 3, \"name\": \"Face\"},\n",
    "    {\"id\": 4, \"name\": \"Face-guard\"},\n",
    "    {\"id\": 5, \"name\": \"Face-mask-medical\"},\n",
    "    {\"id\": 6, \"name\": \"Foot\"},\n",
    "    {\"id\": 7, \"name\": \"Tools\"},\n",
    "    {\"id\": 8, \"name\": \"Glasses\"},\n",
    "    {\"id\": 9, \"name\": \"Gloves\"},\n",
    "    {\"id\": 10, \"name\": \"Helmet\"},\n",
    "    {\"id\": 11, \"name\": \"Hands\"},\n",
    "    {\"id\": 12, \"name\": \"Head\"},\n",
    "    {\"id\": 13, \"name\": \"Medical-suit\"},\n",
    "    {\"id\": 14, \"name\": \"Shoes\"},\n",
    "    {\"id\": 15, \"name\": \"Safety-suit\"},\n",
    "    {\"id\": 16, \"name\": \"Safety-vest\"},\n",
    "]\n",
    "train_processed = valid_processed = test_processed = 0\n",
    "\n",
    "phases = [\"train\", \"valid\"]\n",
    "for phase in phases:\n",
    "    label_dir = \"temp/datasets/{}/labels100\".format(phase)\n",
    "    image_dir = \"temp/datasets/{}/images100\".format(phase)\n",
    "    \n",
    "    res_file = {\n",
    "        \"categories\": categories,\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "    json_file = \"{}100.json\".format(phase)\n",
    "    \n",
    "    annot_count = 0\n",
    "    image_id = 0\n",
    "    processed = 0\n",
    "    if phase == \"test\":\n",
    "        for filename in os.listdir(image_dir):\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img_w, img_h = img.size\n",
    "            res_file[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": img_w,\n",
    "                \"height\": img_h,\n",
    "            })\n",
    "            processed += 1\n",
    "            image_id += 1\n",
    "        test_processed = processed\n",
    "        break\n",
    "        \n",
    "    for filename in os.listdir(label_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            image_extensions = ['.jpeg', '.jpg', '.png']\n",
    "            image_file_name = None\n",
    "            for ext in image_extensions:\n",
    "                image_name = filename.replace('.txt', ext)\n",
    "                if os.path.exists(os.path.join(image_dir, image_name)):\n",
    "                    image_file_name = image_name\n",
    "                    break\n",
    "            if image_file_name is None:\n",
    "                print(f\"Warning: No image found for {filename}\")\n",
    "                continue\n",
    "            \n",
    "            #image_file_name = filename.replace('.txt', '.jpeg')  # Assuming .jpg images\n",
    "            img_path = os.path.join(image_dir, image_file_name)\n",
    "            img = Image.open(img_path)\n",
    "            img_w, img_h = img.size\n",
    "            res_file[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": image_file_name,\n",
    "                \"width\": img_w,\n",
    "                \"height\": img_h,\n",
    "            })\n",
    "            with open(os.path.join(label_dir, filename), 'r') as file:\n",
    "                for line in file:\n",
    "                    parts = line.strip().split()\n",
    "                    category_id = int(parts[0])\n",
    "                    x_center = float(parts[1])\n",
    "                    y_center = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "                    # Convert to COCO bounding box format (x_min, y_min, width, height)\n",
    "                    x_min = x_center - width / 2\n",
    "                    y_min = y_center - height / 2\n",
    "                    res_file[\"annotations\"].append({\n",
    "                        \"id\": annot_count,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"bbox\": [x_min*img_w, y_min*img_h, width*img_w, height*img_h],\n",
    "                        \"area\": width *img_w * height * img_h,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    annot_count += 1\n",
    "            processed += 1\n",
    "            image_id += 1\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json_str = json.dumps(res_file)\n",
    "        f.write(json_str)\n",
    "        #indent 4 or not?\n",
    "        \n",
    "        #with open(output_json, 'w') as json_file:\n",
    "            #json.dump(coco_format, json_file, indent=4)\n",
    "    print(\"Processed {} {} images...\".format(processed, phase))\n",
    "    if phase == \"train\":\n",
    "        train_processed = processed\n",
    "    else:\n",
    "        valid_processed = processed\n",
    "print('train_processed = {}, valid_processed = {}, test_processed = {}'.format(train_processed, valid_processed, test_processed))\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab11bffc-9651-48e0-8756-4b115542dfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 train images...\n",
      "Processed 100 valid images...\n",
      "train_processed = 100, valid_processed = 100, test_processed = 0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python to_coco100.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85655311-d186-4b52-9081-d8b1b162700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4319\n",
      "2160\n",
      "1620\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lable_dir = \"datasets/train/labels\"\n",
    "print(len(os.listdir(lable_dir)))\n",
    "lable_dir = \"datasets/valid/labels\"\n",
    "print(len(os.listdir(lable_dir)))\n",
    "lable_dir = \"datasets/test/images\"\n",
    "print(len(os.listdir(lable_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49f3e19-081d-44ac-b4dc-892ae91aff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_cxcywh_to_xyxy(x):\n",
    "    #x_min, y_min, w, h = x.unbind(1)\n",
    "    x_min, y_min, w, h = x\n",
    "    b = [(x_min), (y_min),\n",
    "         (x_min + w), (y_min + h)]\n",
    "    #return torch.stack(b, dim=1)\n",
    "    return torch.tensor(b)\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    #print(b)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6ca2bd-54ef-41ce-9747-bdae552d40e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting to_coco.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile to_coco.py\n",
    "\n",
    "# modified to produce only 10 valid labels/images json file\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "\n",
    "MAX_N = 100\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"Person\"},\n",
    "    {\"id\": 1, \"name\": \"Ear\"},\n",
    "    {\"id\": 2, \"name\": \"Earmuffs\"},\n",
    "    {\"id\": 3, \"name\": \"Face\"},\n",
    "    {\"id\": 4, \"name\": \"Face-guard\"},\n",
    "    {\"id\": 5, \"name\": \"Face-mask-medical\"},\n",
    "    {\"id\": 6, \"name\": \"Foot\"},\n",
    "    {\"id\": 7, \"name\": \"Tools\"},\n",
    "    {\"id\": 8, \"name\": \"Glasses\"},\n",
    "    {\"id\": 9, \"name\": \"Gloves\"},\n",
    "    {\"id\": 10, \"name\": \"Helmet\"},\n",
    "    {\"id\": 11, \"name\": \"Hands\"},\n",
    "    {\"id\": 12, \"name\": \"Head\"},\n",
    "    {\"id\": 13, \"name\": \"Medical-suit\"},\n",
    "    {\"id\": 14, \"name\": \"Shoes\"},\n",
    "    {\"id\": 15, \"name\": \"Safety-suit\"},\n",
    "    {\"id\": 16, \"name\": \"Safety-vest\"},\n",
    "]\n",
    "\n",
    "train_processed = valid_processed = test_processed = 0\n",
    "phases = [\"valid\"]\n",
    "#phases = [\"train\", \"valid\", \"test\"]\n",
    "for phase in phases:\n",
    "    label_dir = \"datasets/{}/labels10\".format(phase)\n",
    "    image_dir = \"datasets/{}/images10\".format(phase)\n",
    "    \n",
    "    res_file = {\n",
    "        \"categories\": categories,\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "    json_file = \"{}{}.json\".format(phase, \"10\")\n",
    "    print(json_file)\n",
    "    annot_count = 0\n",
    "    image_id = 0\n",
    "    processed = 0\n",
    "    if phase == \"test\":\n",
    "        for filename in os.listdir(image_dir):\n",
    "            img_path = os.path.join(image_dir, filename)\n",
    "            img = Image.open(img_path)\n",
    "            img_w, img_h = img.size\n",
    "            res_file[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": filename,\n",
    "                \"width\": img_w,\n",
    "                \"height\": img_h,\n",
    "            })\n",
    "            processed += 1\n",
    "            image_id += 1\n",
    "        with open(json_file, \"w\") as f:\n",
    "            json_str = json.dumps(res_file)\n",
    "            f.write(json_str)\n",
    "        test_processed = processed\n",
    "        break\n",
    "        \n",
    "    for filename in os.listdir(label_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            image_extensions = ['.jpeg', '.jpg', '.png']\n",
    "            image_file_name = None\n",
    "            for ext in image_extensions:\n",
    "                image_name = filename.replace('.txt', ext)\n",
    "                if os.path.exists(os.path.join(image_dir, image_name)):\n",
    "                    image_file_name = image_name\n",
    "                    break\n",
    "            if image_file_name is None:\n",
    "                print(f\"Warning: No image found for {filename}\")\n",
    "                continue\n",
    "            \n",
    "            #image_file_name = filename.replace('.txt', '.jpeg')  # Assuming .jpg images\n",
    "            img_path = os.path.join(image_dir, image_file_name)\n",
    "            img = Image.open(img_path)\n",
    "            img_w, img_h = img.size\n",
    "            res_file[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": image_file_name,\n",
    "                \"width\": img_w,\n",
    "                \"height\": img_h,\n",
    "            })\n",
    "            with open(os.path.join(label_dir, filename), 'r') as file:\n",
    "                for line in file:\n",
    "                    parts = line.strip().split()\n",
    "                    category_id = int(parts[0])\n",
    "                    x_center = float(parts[1])\n",
    "                    y_center = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "        \n",
    "                    # Convert to COCO bounding box format (x_min, y_min, width, height)\n",
    "                    x_min = x_center - width / 2\n",
    "                    y_min = y_center - height / 2\n",
    "                    res_file[\"annotations\"].append({\n",
    "                        \"id\": annot_count,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": category_id,\n",
    "                        \"bbox\": [x_min*img_w, y_min*img_h, width*img_w, height*img_h],\n",
    "                        \"area\": width * height,\n",
    "                        \"ignore\": 0,\n",
    "                        \"iscrowd\": 0\n",
    "                    })\n",
    "                    #print(res_file[\"annotations\"][annot_count][\"bbox\"])\n",
    "                    annot_count += 1\n",
    "            print(\"Processed {} {} images...\".format(processed, phase))\n",
    "            processed += 1\n",
    "            image_id += 1            \n",
    "    with open('datasets/' + json_file, \"w\") as f:\n",
    "        json_str = json.dumps(res_file)\n",
    "        f.write(json_str)\n",
    "        #indent 4 or not?\n",
    "        \n",
    "        #with open(output_json, 'w') as json_file:\n",
    "            #json.dump(coco_format, json_file, indent=4)\n",
    "    if phase == \"train\":\n",
    "        train_processed = processed\n",
    "    else:\n",
    "        valid_processed = processed\n",
    "print('train_processed = {}, valid_processed = {}, test_processed = {}'.format(train_processed, valid_processed, test_processed))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "923be951-d068-4f40-85f7-0e3d1d983bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid10.json\n",
      "Processed 0 valid images...\n",
      "Processed 1 valid images...\n",
      "Processed 2 valid images...\n",
      "Processed 3 valid images...\n",
      "Processed 4 valid images...\n",
      "Processed 5 valid images...\n",
      "Processed 6 valid images...\n",
      "Processed 7 valid images...\n",
      "Processed 8 valid images...\n",
      "Processed 9 valid images...\n",
      "train_processed = 0, valid_processed = 10, test_processed = 0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python to_coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca1346bc-145a-4d24-a13f-f77fb60c800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_validGT.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_validGT.py\n",
    "\n",
    "# modified to produce only 100 valid labels/images json file\n",
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import itertools\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import scipy.io as sio\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "train_processed = valid_processed = test_processed = 0\n",
    "phases = [\"valid\"]\n",
    "#phases = [\"train\", \"valid\", \"test\"]\n",
    "for phase in phases:\n",
    "    label_dir = \"temp/datasets/{}/labels100\".format(phase)\n",
    "    image_dir = \"temp/datasets/{}/images100\".format(phase)\n",
    "    res_file = {}\n",
    "    \n",
    "    json_file = \"{}{}.json\".format(phase, \"100GT\")\n",
    "    print(json_file)\n",
    "    annot_count = 0\n",
    "    image_id = 0\n",
    "    processed = 0\n",
    "    for filename in os.listdir(label_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            image_extensions = ['.jpeg', '.jpg', '.png']\n",
    "            image_file_name = None\n",
    "            for ext in image_extensions:\n",
    "                image_name = filename.replace('.txt', ext)\n",
    "                if os.path.exists(os.path.join(image_dir, image_name)):\n",
    "                    image_file_name = image_name\n",
    "                    break\n",
    "            if image_file_name is None:\n",
    "                print(f\"Warning: No image found for {filename}\")\n",
    "                continue\n",
    "            img_path = os.path.join(image_dir, image_file_name)\n",
    "            img = Image.open(img_path)\n",
    "            img_w, img_h = img.size\n",
    "            \n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            with open(os.path.join(label_dir, filename), 'r') as file:\n",
    "                for line in file:\n",
    "                    parts = line.strip().split()\n",
    "                    category_id = int(parts[0])\n",
    "                    x_center = float(parts[1])\n",
    "                    y_center = float(parts[2])\n",
    "                    width = float(parts[3])\n",
    "                    height = float(parts[4])\n",
    "        \n",
    "                    # Convert to COCO bounding box format (x_min, y_min, width, height)\n",
    "                    x_min = x_center - width / 2\n",
    "                    y_min = y_center - height / 2\n",
    "                    labels.append(category_id)\n",
    "                    bboxes.append([x_min*img_w, y_min*img_h, width*img_w, height*img_h])\n",
    "            print(\"Processed {} {} images...\".format(processed, phase))\n",
    "            res_file[image_file_name] = {\n",
    "                \"boxes\": bboxes,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "            processed += 1\n",
    "    with open('datasets/' + json_file, \"w\") as f:\n",
    "        json_str = json.dumps(res_file)\n",
    "        f.write(json_str)\n",
    "        #indent 4 or not?\n",
    "        \n",
    "        #with open(output_json, 'w') as json_file:\n",
    "            #json.dump(coco_format, json_file, indent=4)\n",
    "    if phase == \"train\":\n",
    "        train_processed = processed\n",
    "    else:\n",
    "        valid_processed = processed\n",
    "print('train_processed = {}, valid_processed = {}, test_processed = {}'.format(train_processed, valid_processed, test_processed))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afbf3bf-3eb9-4125-a6a5-e819f55adae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid100GT.json\n",
      "Processed 0 valid images...\n",
      "Processed 1 valid images...\n",
      "Processed 2 valid images...\n",
      "Processed 3 valid images...\n",
      "Processed 4 valid images...\n",
      "Processed 5 valid images...\n",
      "Processed 6 valid images...\n",
      "Processed 7 valid images...\n",
      "Processed 8 valid images...\n",
      "Processed 9 valid images...\n",
      "Processed 10 valid images...\n",
      "Processed 11 valid images...\n",
      "Processed 12 valid images...\n",
      "Processed 13 valid images...\n",
      "Processed 14 valid images...\n",
      "Processed 15 valid images...\n",
      "Processed 16 valid images...\n",
      "Processed 17 valid images...\n",
      "Processed 18 valid images...\n",
      "Processed 19 valid images...\n",
      "Processed 20 valid images...\n",
      "Processed 21 valid images...\n",
      "Processed 22 valid images...\n",
      "Processed 23 valid images...\n",
      "Processed 24 valid images...\n",
      "Processed 25 valid images...\n",
      "Processed 26 valid images...\n",
      "Processed 27 valid images...\n",
      "Processed 28 valid images...\n",
      "Processed 29 valid images...\n",
      "Processed 30 valid images...\n",
      "Processed 31 valid images...\n",
      "Processed 32 valid images...\n",
      "Processed 33 valid images...\n",
      "Processed 34 valid images...\n",
      "Processed 35 valid images...\n",
      "Processed 36 valid images...\n",
      "Processed 37 valid images...\n",
      "Processed 38 valid images...\n",
      "Processed 39 valid images...\n",
      "Processed 40 valid images...\n",
      "Processed 41 valid images...\n",
      "Processed 42 valid images...\n",
      "Processed 43 valid images...\n",
      "Processed 44 valid images...\n",
      "Processed 45 valid images...\n",
      "Processed 46 valid images...\n",
      "Processed 47 valid images...\n",
      "Processed 48 valid images...\n",
      "Processed 49 valid images...\n",
      "Processed 50 valid images...\n",
      "Processed 51 valid images...\n",
      "Processed 52 valid images...\n",
      "Processed 53 valid images...\n",
      "Processed 54 valid images...\n",
      "Processed 55 valid images...\n",
      "Processed 56 valid images...\n",
      "Processed 57 valid images...\n",
      "Processed 58 valid images...\n",
      "Processed 59 valid images...\n",
      "Processed 60 valid images...\n",
      "Processed 61 valid images...\n",
      "Processed 62 valid images...\n",
      "Processed 63 valid images...\n",
      "Processed 64 valid images...\n",
      "Processed 65 valid images...\n",
      "Processed 66 valid images...\n",
      "Processed 67 valid images...\n",
      "Processed 68 valid images...\n",
      "Processed 69 valid images...\n",
      "Processed 70 valid images...\n",
      "Processed 71 valid images...\n",
      "Processed 72 valid images...\n",
      "Processed 73 valid images...\n",
      "Processed 74 valid images...\n",
      "Processed 75 valid images...\n",
      "Processed 76 valid images...\n",
      "Processed 77 valid images...\n",
      "Processed 78 valid images...\n",
      "Processed 79 valid images...\n",
      "Processed 80 valid images...\n",
      "Processed 81 valid images...\n",
      "Processed 82 valid images...\n",
      "Processed 83 valid images...\n",
      "Processed 84 valid images...\n",
      "Processed 85 valid images...\n",
      "Processed 86 valid images...\n",
      "Processed 87 valid images...\n",
      "Processed 88 valid images...\n",
      "Processed 89 valid images...\n",
      "Processed 90 valid images...\n",
      "Processed 91 valid images...\n",
      "Processed 92 valid images...\n",
      "Processed 93 valid images...\n",
      "Processed 94 valid images...\n",
      "Processed 95 valid images...\n",
      "Processed 96 valid images...\n",
      "Processed 97 valid images...\n",
      "Processed 98 valid images...\n",
      "Processed 99 valid images...\n",
      "train_processed = 0, valid_processed = 100, test_processed = 0\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "!python get_validGT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "282a2349-36ba-44d5-a90c-efd0cde8c393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 100 files to 'datasets/train/images100'.\n",
      "Copied 100 files to 'datasets/train/labels100'.\n",
      "Copied 100 files to 'datasets/valid/images100'.\n",
      "Copied 100 files to 'datasets/valid/labels100'.\n",
      "Copied 100 files to 'datasets/test/images100'.\n"
     ]
    }
   ],
   "source": [
    "#copying 100 sample data, for upload and easy wget access\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source and destination directories\n",
    "phases = ['train', 'valid']\n",
    "for phase in phases:\n",
    "    source_dir = 'datasets/{}/images'.format(phase)\n",
    "    destination_dir = 'datasets/{}/images100'.format(phase)\n",
    "    \n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    # List files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    \n",
    "    # Copy the first 100 files\n",
    "    for i, filename in enumerate(files):\n",
    "        if i >= 100:\n",
    "            break  # Stop after copying 100 files\n",
    "    \n",
    "        # Construct full file paths\n",
    "        src_file = os.path.join(source_dir, filename)\n",
    "        dst_file = os.path.join(destination_dir, filename)\n",
    "    \n",
    "        # Copy the file\n",
    "        shutil.copy(src_file, dst_file)\n",
    "    \n",
    "    print(f\"Copied {min(100, len(files))} files to '{destination_dir}'.\")\n",
    "\n",
    "    source_dir = 'datasets/{}/labels'.format(phase)\n",
    "    destination_dir = 'datasets/{}/labels100'.format(phase)\n",
    "    \n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    # List files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    \n",
    "    # Copy the first 100 files\n",
    "    for i, filename in enumerate(files):\n",
    "        if i >= 100:\n",
    "            break  # Stop after copying 100 files\n",
    "    \n",
    "        # Construct full file paths\n",
    "        src_file = os.path.join(source_dir, filename)\n",
    "        dst_file = os.path.join(destination_dir, filename)\n",
    "    \n",
    "        # Copy the file\n",
    "        shutil.copy(src_file, dst_file)\n",
    "    \n",
    "    print(f\"Copied {min(100, len(files))} files to '{destination_dir}'.\")\n",
    "\n",
    "phase = 'test'\n",
    "source_dir = 'datasets/{}/images'.format(phase)\n",
    "destination_dir = 'datasets/{}/images100'.format(phase)\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# List files in the source directory\n",
    "files = os.listdir(source_dir)\n",
    "\n",
    "# Copy the first 100 files\n",
    "for i, filename in enumerate(files):\n",
    "    if i >= 100:\n",
    "        break  # Stop after copying 100 files\n",
    "\n",
    "    # Construct full file paths\n",
    "    src_file = os.path.join(source_dir, filename)\n",
    "    dst_file = os.path.join(destination_dir, filename)\n",
    "\n",
    "    # Copy the file\n",
    "    shutil.copy(src_file, dst_file)\n",
    "\n",
    "print(f\"Copied {min(100, len(files))} files to '{destination_dir}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71da4d59-5593-4166-bb7d-d57483095e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir('datasets/{}/images100'.format(phase))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
